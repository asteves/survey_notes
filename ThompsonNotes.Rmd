---
title: "Sampling Notes" 
---

## Chapter 1 

**Sampling**: Selecting some part of a population to observe so that we can estimate a population parameter 

The basic setup of sampling: 

- Population is known and of finite size *N* units $i \in \{1,...,N\}$
- Each unit has an associated value of interest (y-value) which is fixed and unknown 

**Sampling Design**: The procedure through which the sample of units is selected from the population 

## Chapter 2 

### Simple Random Sampling

**Simple Random Sampling**: A sampling design in which *n* distinct units are selected from *N* units in the population in such a way that every possible combination of *n* units is equally likely to be in the selected sample. 

SRS is also known as random sampling without replacement. 

The inclusion probability for SRS is the same for all units. The probability that the $i^{th}$ unit of the population is included is $\pi_i = \frac{n}{N}$. A SRS **guarantees** that each possible sample of *n* units for all units. 

We estimate the population mean with SRS as follows: 

- $\bar{y}$ is unbiased for the population mean $\mu$. We define $\bar{y} = \frac{1}{n}\sum_{i=1}^{n}y_i$

- $s^2$ is unbiased for the population variance $\sigma^2$. We define $s^2 = \frac{1}{n-1}\sum_{i=1}^{n}(y_i - \bar{y})^2$

- The variance of $\bar{y}$ with SRS is $V[\widehat{\bar{y}}] = \frac{N-n}{N}\frac{s^2}{n}$. The standard error is defined by taking the square root of this quantity. 

- To estimate the population total an unbiased estimator $\tau = N\mu$. An unbiased estimator of $V[\tau]$ is $N(N-n)\frac{s^2}{n}$

### Underlying ideas for SRS 

$\bar{y}$ is a random variable whose expectation is the population parameter where the expectation is taken over all possible samples. This makes $\bar{y}$ *design-unbiased* and this does not depend on any assumptions about the population. The variance estimates are design unbiased as well. 

### Derivations for SRS 

$$\begin{aligned}
E[\bar{y}] &= \sum\bar{y_s}P(s) \\
E[\bar{y}] &= \frac{1}{n} \sum y_i\frac{\binom{N-1}{n-1}}{\binom{N}{n}} \\
E[\bar{y}] &= \frac{1}{N}\sum y_i
\end{aligned}$$

An alternative derivation 

$$\begin{aligned}
E[\bar{y}] &= \frac{1}{n}\sum_{i=1}^{N}y_iE[z_i] \\
E[\bar{y}] &= \frac{1}{n}\sum_{i = 1}^{N}y_i\frac{n}{N}\\
E[\bar{y}] &= \frac{1}{n}\frac{n}{N}\sum_{i=1}^{N}y_i \\
E[\bar{y}] &= \frac{1}{N}\sum_{i=1}^{N} y_i
\end{aligned}$$
To derive the variance of the sample mean, see pages 21-22. 

### Examples and Exercises 

Example 1 p. 16 

```{r}
# sampling functions 
sample_mean <- function(vec){
  return(sum(vec)/length(vec))
}

sample_var <- function(vec){
  ybar <- sample_mean(vec)
  n <- length(vec)
  # finite population correction
  return((1/(n-1))*sum((vec - ybar)^2))
}

var_sample_mean <- function(vec, N){
  n <- length(vec)
  # finite population correction 
  fpc <- (N-n)/N
  return(fpc*(sample_var(vec)/n))
}

se_mean <- function(vec, N){
  return(sqrt(var_sample_mean(vec,N)))
}

estimate_pop_total <- function(vec, N){
  return(N*sample_mean(vec))
}

var_pop_total <- function(vec, N){
  return(N^2*var_sample_mean(vec,N))
}
```

```{r}
## Caribou Example 1 page 16
N <- 286
sample <- c(1,50,21,98,2,36,4,29,7,15,86,10,21,5,4)

# Get sample mean, sample variance, variance and standard error 
# of the sample mean, the population total estimate, variance of
# the population total estimate and standard error of pop total 
# estimate
sample_mean(sample)
sample_var(sample)
var_sample_mean(sample, N)
se_mean(sample,N)
estimate_pop_total(sample,N)
var_pop_total(sample,N)
sqrt(var_pop_total(sample,N))
```

Example 2 p.18 

```{r}
## All possible samples 
lectures <- tibble::tibble(
  unit = 1:4,
  y_i = c(10,17,13,20)
)

## all possible sample of 2 from size 4. Will equal 6 
choose(4,2)

## Generate the eight unique samples 
samples <-combn(c(1,2,3,4),2)

mat <- matrix(data = NA, nrow = 12, ncol = 4)

for(i in seq(1,12,by =2)){
  ys <- lectures$y_i[c(samples[i],samples[i+1])]
  ybar <- sample_mean(ys)
  pop_total <- estimate_pop_total(ys,N = 4)
  s2 <- sample_var(vec = ys)
  var_ybar <- var_pop_total(ys, 4)
  mat <- rbind(mat, c(ybar, pop_total, s2, var_ybar))
}

mat <- mat[rowSums(is.na(mat)) !=ncol(mat),]

## Get Expected totals
colMeans(mat)
```

Example 3 p. 20 

```{r}
sample_RS <- c(2,4,0,4,5)

yn_bar <- mean(sample_RS)

yv_bar <- mean(unique(sample_RS))
yv_bar
```

Example 4 p. 32 

```{r}
fir <- boot::fir

y <- fir$count

## Get true population mean 
mu <- mean(y)

# Simulate repeated draws 
N <- 10000
ybar <- vector(mode = "logical",length = N)

for(i in 1:N){
  ybar[i] <- mean(y[sample(1:50, 5)])
}
## Expected value of ybar 
mean(ybar)

## MSE 
mean((ybar - mu)^2)
```

Exercise 2 p.36

```{r}
# A SRS of 10 households is selected from a population of 100 households. 
N <- 100
y <- c(2,5,1,4,4,3,2,5,2,3)

## Estimate the total number of people in the population and variance of the estimator 
estimate_pop_total(y, N)

var_pop_total(y, N)

## Estimate the mean number of people per household and estimate the variance of the estimator 
sample_mean(y)

var_sample_mean(y, N)
```

Exercise 3 p. 36 

```{r}
## Consider a population N = 5
N <- 5
y <- c(3,1,0,1,5)
i <- 1:5

# Consider a SRS with a sample size n = 3.
n <- 3
## What is the probability of each sample being the one selected 
pi <- n/N
pi

## List every possible sample of size n  = 3
samples <- t(combn(i, 3))
samples

## Give values of the population parameters mu tau sigma^2
pop_mean <- mean(y)
pop_sum <- sum(y)
pop_var <- var(y)*(N/N-1)
pop_med <- median(y)

# B) For each sample, compute the sample mean ybar and the sample median demonstrate that the sample mean is unbiased for the population 
ybar <- vector(mode = "numeric", length = 10)
ymed <- vector(mode = "numeric", length = 10)
for(i in 1:length(ybar)){
  samp <- y[as.vector(samples[i,])]
  ybar[i] <- mean(samp)
  ymed[i] <- median(samp)
}
## sample mean is an unbiased estimator 
mean(ybar) - pop_mean == 0

## sample median is not unbiased
mean(ymed) - pop_med == 0
```

Exercise 4 p.36 

Show that $E[s^2] = \sigma^2$ 

$$\begin{aligned}
E[s^2] &= E[\frac{n}{n-1}(\bar{X^2}-\bar{X}^2)] \\
E[s^2] &= \frac{n}{n-1}E[\bar{X^2}-\bar{X}^2)]\\
E[s^2] &= \frac{n}{n-1}\left(\frac{n-1}{n}\sigma^2 \right) \\
E[s^2] &= \sigma^2
\end{aligned}$$
Exercise 6 p.37 

```{r}

trees <- datasets::trees
y <- trees$Volume
mu <- mean(y)
N <- length(y)
n <- 10

# Repeat the simulation exercise using n = 10 and n = 15 
ybar <- vector(mode = "logical", length = 10000)
var_y <- vector(mode = "logical", length = 10000)
set.seed(1234)
for(i in 1:length(ybar)){
  smp <- y[sample(1:N, n)]
  ybar[i] <- mean(smp)
  var_y[i] <- var_sample_mean(smp, 31)
  
}
## E_y
mean(ybar)

## V[ybar]
mean(var_y)

## MSE 
mean((ybar - mu)^2)

### B use n = 15 
n = 15
ybar <- vector(mode = "logical", length = 10000)
var_y <- vector(mode = "logical", length = 10000)
set.seed(1234)
for(i in 1:length(ybar)){
  smp <- y[sample(1:N, n)]
  ybar[i] <- mean(smp)
  var_y[i] <- var_sample_mean(smp, 31)
  
}
## E_y
mean(ybar)

## V[ybar]
mean(var_y)

## MSE 
mean((ybar - mu)^2)
```

### Random Sampling with Replacement

**Random Sampling with Replacement**: A sample of *n* units selected from a population of size *N*, returning each unit to the population after it has been drawn. 

- Every possible sequence of *n* units has equal probability under the design. 

- The plug in estimator $\bar{y_n} = \frac{1}{n}\sum_{i=1}^{n} y_i$. 

- The variance of the plug-in sample mean estimator is $V[\hat{\bar{y_n}}] = \frac{s^2}{n}$ which is unbiased for the parameter $V[\bar{y_n}] = \frac{N-1}{nN}\sigma^2$.

The variance of the SRS sample mean is in general lower than the variance of the sample mean taken via random sampling with replacement. 

In random sampling with replacement $\bar{y_n}$ depends on the number of times each unit is selected. This is why the notation is a bit different. Two surveys that observe the same distinct set of units but with different repeats in the sample will generally yield different estimates. 

**Effective sample size**: In random sampling with replacement, this is the number of distinct units in the sample (denoted v). 

- $\bar{y_v} = \frac{1}{v}\sum_{i=1}^{n}y_i$. This quantity is an unbiased estimator of the population mean. 

## Chapter 3: Confidence Intervals

Let *I* represent a confidence interval for the population mean $\mu$. Choose a small number $\alpha$ as the allowable probability of error such that the procedure has a probability $P(\mu \in I) = 1-\alpha$. 

*I* is a random variable and the endpoints of *I* will vary from sample to sample. Conversely, $\mu$ is fixed and unknown parameter. 

Under SRS a $1-\alpha$ CI means that for $1-\alpha$ of the possible samples, the interval contains the true value of the population mean. 

An approximate $100(1-\alpha)\%$ CI for $\mu$ is 

$$\bar{y} \pm t\sqrt{\frac{N-n}{N}\frac{s^2}{n}}$$
where t is the upper $\frac{\alpha}{2}$ point of a t-distribution with n-1 degrees of freedom. 

We rely here for this on the finite population central limit theorem which shows that the distribution will converge to a standard normal distribution as *n* and *N* grow large. 

### Exercises 

Exercise 3 p.51

```{r}
## Consider a population N = 5
N <- 5
n <- 3
y <- c(3,1,0,1,5)
i <- 1:5

## List every possible sample of size n  = 3
samples <- t(combn(i, n))
samples

## Give values of the population parameters mu tau sigma^2
pop_mean <- mean(y)
pop_var <- var(y)
t <- qt(0.975, df = n -1)

# B) For each sample, compute the sample mean ybar and the sample median demonstrate that the sample mean is unbiased for the population 
ybar <- vector(mode = "numeric", length = 10)
var_ybar <- vector(mode = "numeric", length = 10)
s2 <- vector(mode = "numeric", length = 10)

## Needed for part 3 to compute coverage 
lwr_bound <- vector(mode = "numeric", length = 10)
upp_bound <- vector(mode = "numeric", length = 10)

## Simulate all possible samples 
for(i in 1:length(ybar)){
  samp <- y[as.vector(samples[i,])]
  ybar[i] <- mean(samp)
  s2[i] <- sample_var(samp)
  var_ybar[i] <- var_sample_mean(samp, N)
  print(samp)
  lwr_bound[i] <- mean(samp) -
    t*sqrt(var_sample_mean(samp, 5))/sqrt(n)
  upp_bound[i] <- mean(samp) +
    t*sqrt(var_sample_mean(samp, 5))/sqrt(n)
}

## sample mean is an unbiased estimator 
mean(ybar) - pop_mean == 0

## sample var is unbiased
mean(s2) - pop_var == 0

## sample square root is not 
mean(sqrt(s2))- sqrt(pop_var) == 0

## Coverage function 
coverage <- function(theta, lb, ub){
  if(lb <= theta & theta <= ub){
    return(TRUE)
  }
  else{
    return(FALSE)
  }
}
covered <- vector(mode = "numeric", length = 10)
for(i in 1:10){
  covered[i]<-coverage(pop_mean, lwr_bound[i], upp_bound[i])
}

# Coverage is 90% of samples
mean(covered)

```

## Chapter 4: Sample Size 

The first question when planning a survey is what sample size should be used. We estimate a statistic $\hat{\theta}$ that is an unbiased estimator of $\theta$ which is the population parameter of interest. 

We want to choose a maximum allowable difference *d* between the true parameter and the estimate and allow for some small probability $\alpha$ that the error could exceed our chosen specification. 

This means that we choose our sample size *n* such that $P(|\hat{\theta} - \theta) > d < \alpha$

**Sample size for estimating a population mean** 

Under SRS 

$$V[\bar{y}] = (N - n)\frac{\sigma^2}{Nn}$$ 

Thus $d = z \sqrt{V[\bar{y}]}$ assuming that that our estimator is normally distributed, which is the case for a population mean. 

Solving for *n* 

$$\begin{aligned}
n &= \frac{1}{\frac{d^2\sigma^2}{t^2} + \frac{1}{N}} \\
n &= \frac{1}{\frac{1}{n_0} + \frac{1}{N}}
\end{aligned}$$

with an appropriate substitution. The only difference to get a population total is to multiply $n_0$ by $N^2$ in the above formula. 

Both of these formulas presume knowledge of the population variance, which is generally unknown. To get an estimate, we tend to either guess completely or use the results of a previous survey. 

If we ignore the finite population correct, then we only have to calculate $n_0$. This sample size will always be larger than the sample size with a finite population correction. 

**Sample Size for Relative Precision** 

The formula is now 

$$n = \frac{1}{\frac{r^2\mu^2}{t^2\sigma^2} + \frac{1}{N}}$$
where *r* represents the relative error of interest. In this formula the population quantity on which sample size depends when the desire is to control relative precision is the coefficient of variation in the population. 

### Exercises 

Exercise 1 p.56 
```{r}
sample_size_formula<- function(N, sigma_2, t,d){
  n0_num <- N^2*sigma_2*t^2
  n0_den <- d^2
  return(n0_num/n0_den)
}

ssf_wfpc <- function(N, sigma_2, t, d){
  n0 = sample_size_formula(N, sigma_2, t, d)
  return(1/ ((1/n0) + 1/N))
}
d <- c(500, 1000, 2000)

# Since n is large, we can get away with this 
# because it converges to a normal distribution
t <- 1.96

# A more complete computation is 
alpha <- 0.05
# qt() is the quantile function for the t-distribution
# see ?qt()
t_better <- qt(1- (alpha/2), df = d-1)


## Parameters from problem. N is total population. sigma_2 = $\sigma^2$ = variance given for population
N <- 1000
sigma_2 <- 45

## ignore FPC
for(i in 1:length(d)){
  n <- sample_size_formula(N = N, sigma_2 = sigma_2, t = t_better[i], d = d[i])
  print(round(n))
}

## With FPC 
for(i in 1:length(d)){
  print(round(ssf_wfpc(N,sigma_2,t_better[i],d[i])))
}

```

## Chapter 5: Estimating Proportions, Ratios, and Subpopulation Means 

We can use the the same formulas as before, but some special features of note: 
- Formulas simplify considerably with attribute data
- Exact Confidence Intervals are possible
- A sample size sufficient for a desired absolute precision may be chosen without any information on population parameters 

### Estimating a population proportion 

$$p = \frac{1}{N}sum_{i=1}^{N}y_i = \mu$$ 

Finite population variance 

$$s^2 = \frac{n}{n-1}\hat{p}(1-\hat{p})$$

Variance of the estimator 

$$V[\hat{p}] = \frac{N-n}{N-1}\frac{\hat{p}(1-\hat{p})}{n}$$
The first term is the FPC (I think) so we can ignore it to get an estimator that is slightly too large. 

Confidence intervals work as expected 

$$\hat{p} \pm \sqrt{V[\hat{p}]}$$

### Sample size for estimating a proportion 

Sample size requirements when we ignore the finite population correction 

$$n_0 = \frac{z^2p(1-p)}{d^2}$$

When including the FPC 

$$n = \frac{1}{\frac{1}{n_0} + \frac{1}{N}}$$
where $n_0$ is defined as above. 

Like before these formulas depend on an estimate of $p$. If $p$ is unknown use $p = 0.5$ because this is the value for which the function above takes on its maximum value as a function of p. To see this, simply look at the part of the function that includes $p$. 

$$\begin{aligned}
\frac{d}{dp} &= [p(1-p)]' \\
0 &= 1-2p \\
p &= \frac{1}{2}
\end{aligned}$$

### Estimating a Ratio 

The population ratio is estimated by dividing the total y-values by the total x-values in the samples 

$$r = \frac{\sum_{i=1}^{n}y_i}{\sum_{i=1}^{n}x_i} = \frac{\bar{y}}{\bar{x}}$$
Ratio estimators are not design unbiased. More on this in [Chapter 7](#chap7) and [Chapter 12](#chap12)

### Estimating a Mean, total, or Population of a subpopulation

An estimator for sub-population with characteristic $a_i$ in a sample of $n$ with attribute $n_1$ is: 

$$\hat{p_i} = \frac{a_i}{n_i}$$

This proportion has two random variables: $a_i$ and $n_i$. To estimate the sub-population mean, we have *N* units in a population. Let $N_k$ be the number belonging to the $k^{th}$ sub-population. The variable of interest $y_{ik}$. 

The total is:

$$\tau_k = \sum_{i=1}^{N}y_{ki}$$
The estimator of the mean is design unbiased 

$$\bar{y}_k =\frac{1}{n_k} \sum_{i=1}^{n_k}y_{ki}$$

Proof sketch. 

1. Condition on the domain sample size $n_k$. Given $n_k$ every possible combination of $n_k$ of the $N_k$ sub-population units has equal probability of being selected via SRS. 

2. Given (1), $\bar{y}_k$ behaves as the sample mean of a SRS of $n_k$ from $N_k$: $E[\bar{y}_k|n_k] = \mu_k$

3. $E[E[\bar{y}_k|n_k]] = E[\bar{y}_k] = \mu_k$

The variance of the subpopulation mean 

$$\begin{aligned}
V[\bar{y}_k] &= E[V[\bar{y}_k|n_k]] + V[E[\bar{y}|n_k]] \\
V[\bar{y}_k] &= E[V[\bar{y}_k|n_k]] + V[\mu_k] \\
V[\bar{y}_k] &= E[V[\bar{y}_k|n_k]]
\end{aligned}$$

Given SRS 

$$V[\bar{y}_k | n_k] = \sigma^2_k\left(\frac{1}{n_k} - \frac{1}{N_k}\right)$$

where $\sigma^2_k$ is the population variance for units in the $k^{th}$ population. $\sigma^2_k = \frac{1}{N_k -1}\sum_{i=1}^{N_k}(y_{ki} - \mu_k)^2$

The unconditional variance for the estimator is thus: 

$$V[\bar{y}_k] = \frac{N_k - n_k}{N_k n_k}s^2_k$$

where $s^2_k$ is the sample variance of the sub-population of interest. Confidence intervals are found in the usual way. If the sub-population is unknown, we replace the FPC with its expected value $\frac{N-n}{N}$

### Exercises 

Exercise 1 p. 66

```{r}
# Estimate the population proportion in favor and give
# a 95% CI for population proportion 
# SRS of 1200 552 reported in favor 
p_hat <- 552/1200

v_phat <- function(n, p){
  return((p*(1-p))/n)
}

N <- 1800000
n <- 1200

lwr <- round(p_hat - 1.96*sqrt(v_phat(n, p_hat)),2)
upp <- round(p_hat + 1.96*sqrt(v_phat(n, p_hat)),2)

print(paste(p_hat, "with CI", lwr, upp))
```

Exercise 3 p. 66
```{r}

## What sample size is required to estimate the proportion of people with blood type O in a population of 1500 people to be within 0.02 of the true proportion with 95% confidence? Assume no prior knowledge about the proportion.

sample_size_prop_nofpc <- function(p, d){
  n0 <- (1.96^2*p*(1-p))/d^2
  return(n0)
}

N <- 1500
p <- .5
d <- 0.02

## Without FPC 
sample_size_prop_nofpc(p, d)

## With FPC 
round((1)/(1/sample_size_prop_nofpc(p,d) + 1/1500))

```

